{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  Job Description  Company Name\n",
      "Industry                                                       \n",
      "-1                                            353           352\n",
      "IT Services                                   325           325\n",
      "Staffing & Outsourcing                        323           323\n",
      "Health Care Services & Hospitals              151           151\n",
      "Consulting                                    111           111\n",
      "...                                           ...           ...\n",
      "Chemical Manufacturing                          1             1\n",
      "Pet & Pet Supplies Stores                       1             1\n",
      "Consumer Product Rental                         1             1\n",
      "Metals Brokers                                  1             1\n",
      "News Outlet                                     1             1\n",
      "\n",
      "[89 rows x 2 columns]\n",
      "Processing... ../processFiles/trainCatFiles/cat0_Advertising&Marketing.txt\n",
      "Length is:  21227 ['About', 'Known', 'Known', 'is', 'a']\n",
      "Processing... ../processFiles/trainCatFiles/cat1_Banks&CreditUnions.txt\n",
      "Length is:  32003 ['Job', 'Description:', 'Within', 'GMC,', 'Data']\n",
      "Processing... ../processFiles/trainCatFiles/cat2_ComputerHardware&Software.txt\n",
      "Length is:  50038 ['If', 'interested,', 'please', 'reach', 'out']\n",
      "Processing... ../processFiles/trainCatFiles/cat3_Consulting.txt\n",
      "Length is:  48362 ['About', 'Us:', 'NYSTEC', 'is', 'a']\n",
      "Processing... ../processFiles/trainCatFiles/cat4_EnterpriseSoftware&NetworkSolutions.txt\n",
      "Length is:  35454 ['Data', 'Analyst', 'Jersey', 'City,', 'NJ']\n",
      "Processing... ../processFiles/trainCatFiles/cat5_HealthCareServices&Hospitals.txt\n",
      "Length is:  84819 ['Overview', 'Provides', 'analytical', 'and', 'technical']\n",
      "Processing... ../processFiles/trainCatFiles/cat6_ITServices.txt\n",
      "Length is:  96150 ['Requisition', 'NumberRR-0001939', 'Remote:Yes', 'We', 'collaborate.']\n",
      "Processing... ../processFiles/trainCatFiles/cat7_Internet.txt\n",
      "Length is:  34604 ['We√¢\\x80\\x99re', 'looking', 'for', 'a', 'Senior']\n",
      "Processing... ../processFiles/trainCatFiles/cat8_InvestmentBanking&AssetManagement.txt\n",
      "Length is:  41725 ['About', 'Cubist', 'Cubist', 'Systematic', 'Strategies']\n",
      "Processing... ../processFiles/trainCatFiles/cat9_Staffing&Outsourcing.txt\n",
      "Length is:  107734 ['Title:', 'Data', 'Analyst', 'Position:', '1+']\n",
      "/Users/arnabborah/Documents/repositories/textclusteringDBSCAN\n",
      "IDF processing... /Users/arnabborah/Documents/repositories/textclusteringDBSCAN/processFiles/trainCatFiles/cat1_Banks&CreditUnions.txt\n",
      "IDF processing... /Users/arnabborah/Documents/repositories/textclusteringDBSCAN/processFiles/trainCatFiles/cat8_InvestmentBanking&AssetManagement.txt\n",
      "IDF processing... /Users/arnabborah/Documents/repositories/textclusteringDBSCAN/processFiles/trainCatFiles/cat0_Advertising&Marketing.txt\n",
      "IDF processing... /Users/arnabborah/Documents/repositories/textclusteringDBSCAN/processFiles/trainCatFiles/cat7_Internet.txt\n",
      "IDF processing... /Users/arnabborah/Documents/repositories/textclusteringDBSCAN/processFiles/trainCatFiles/cat6_ITServices.txt\n",
      "IDF processing... /Users/arnabborah/Documents/repositories/textclusteringDBSCAN/processFiles/trainCatFiles/cat5_HealthCareServices&Hospitals.txt\n",
      "IDF processing... /Users/arnabborah/Documents/repositories/textclusteringDBSCAN/processFiles/trainCatFiles/cat3_Consulting.txt\n",
      "IDF processing... /Users/arnabborah/Documents/repositories/textclusteringDBSCAN/processFiles/trainCatFiles/cat9_Staffing&Outsourcing.txt\n",
      "IDF processing... /Users/arnabborah/Documents/repositories/textclusteringDBSCAN/processFiles/trainCatFiles/cat4_EnterpriseSoftware&NetworkSolutions.txt\n",
      "IDF processing... /Users/arnabborah/Documents/repositories/textclusteringDBSCAN/processFiles/trainCatFiles/cat2_ComputerHardware&Software.txt\n",
      "Created list of terms for IDF matrix with 50605  terms i.e.  6.732754675209946  of total words!\n",
      "Computing inverse document frequencies for  50605 terms!\n",
      "[ ================================================== ] 100.00%file: /Users/arnabborah/Documents/repositories/textclusteringDBSCAN/processFiles/trainCatFiles/cat1_Banks&CreditUnions.txt\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'getTermFreq' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-6d19c7b06f86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0mft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflingCategoricalTFIDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0mallfnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetallfilenames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Users/arnabborah/Documents/repositories/textclusteringDBSCAN/processFiles/trainCatFiles\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m \u001b[0mft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomputeTFIDFallfiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallfnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-6d19c7b06f86>\u001b[0m in \u001b[0;36mcomputeTFIDFallfiles\u001b[0;34m(self, flist)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/Users/arnabborah/Documents/repositories/textclusteringDBSCAN/processFiles/trainCatFiles/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mflist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"file:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0mtfidfMAT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_tfidf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtfidfMatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidfMAT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0mprog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlenflist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-6d19c7b06f86>\u001b[0m in \u001b[0;36mcompute_tfidf\u001b[0;34m(self, fileIndex, filename)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mwordsInFile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreadfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwordsInFile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mtf_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetTermFreq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileIndex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0midf_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midfMatrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mtfidf_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_final\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0midf_final\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'getTermFreq' is not defined"
     ]
    }
   ],
   "source": [
    "# generate and save characteristic file from text column of csv file and a categorical variable\n",
    "\n",
    "from imp import reload\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import nltk,re,pprint\n",
    "import sys,glob,os\n",
    "import operator, string, argparse, math\n",
    "\n",
    "def drawProgressBar(percent, barLen = 50):\t\t\t#just a progress bar so that you dont lose patience\n",
    "    sys.stdout.write(\"\\r\")\n",
    "    progress = \"\"\n",
    "    for i in range(barLen):\n",
    "        if i<int(barLen * percent):\n",
    "            progress += \"=\"\n",
    "        else:\n",
    "            progress += \" \"\n",
    "    sys.stdout.write(\"[ %s ] %.2f%%\" % (progress, percent * 100))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "class dataProcessor:\n",
    "    def __init__(self, fname, keep_factors = ['Job Description', 'Company Name', 'Industry'], group_column = 'Industry'):\n",
    "        self.dataInitial = pd.read_csv(fname, encoding=\"latin\")\n",
    "        self.dataInitialSmall = self.dataInitial[['Job Description', 'Company Name', 'Industry']]\n",
    "        self.swords = set(stopwords.words('english'))\n",
    "        #print(len(self.swords),\"stopwords present!\")\n",
    "        self.dataInitialGrouped = self.dataInitialSmall.groupby([group_column]).count()\n",
    "        pd.set_option('display.max_rows', 50)\n",
    "        print(self.dataInitialGrouped.sort_values(by=['Job Description'], ascending=False))\n",
    "        \n",
    "    def customProcessData(self):\n",
    "        self.dataTopCat = self.dataInitialSmall[self.dataInitialSmall[\"Industry\"].isin(['IT Services', 'Staffing & Outsourcing', 'Health Care Services & Hospitals', 'Consulting',\n",
    "'Computer Hardware & Software', 'Investment Banking & Asset Management', 'Enterprise Software & Network Solutions', 'Internet',\n",
    "'Banks & Credit Unions', 'Advertising & Marketing'])]\n",
    "        self.dataTopCatGrouped = self.dataTopCat.groupby(['Industry'])['Job Description'].apply(lambda x: ','.join(x)).reset_index()\n",
    "        self.dataTopCatGrouped.reset_index()\n",
    "\n",
    "    # pipeline for purifying the text, write-pipeline, so just output filename can be provided\n",
    "    def rem_stop_punct(self,originalText, ofilename):\n",
    "        splittedText = originalText.split()\n",
    "        lenl = len(splittedText)\n",
    "        print(\"Length is: \",lenl, splittedText[:5])\n",
    "        ofile = open(ofilename,'a')\n",
    "        \n",
    "        for r in range(lenl):\n",
    "            linex = splittedText[r]\n",
    "            linex2 = \"\".join(c for c in linex if c not in ('!','.',':',',','?',';','``','&','-','\"','(',')','[',']','0','1','2','3','4','5','6','7','8','9'))\n",
    "            linex3 = linex2.split()\n",
    "            #prog=(r+1)/len(rawlines)\n",
    "            for s in range(len(linex3)):\n",
    "                noword = linex3[s].lower()\n",
    "                if noword not in self.swords:\n",
    "                    ofile.write(noword)\n",
    "                    ofile.write(\" \")  \n",
    "\n",
    "class flingCategoricalTFIDF:\n",
    "    def __init__(self):\n",
    "        self.nom = 0\n",
    "        self.allfiles = []\n",
    "        self.tfidfMatrix = []\n",
    "        self.tfmatrixAllfiles = []\n",
    "        self.termsforIDF = []\n",
    "        self.idfMatrix = {}\n",
    "        self.termsforIDF = []\n",
    "        self.computed_tfmatrix = 0\n",
    "        self.computed_idfmatrix = 0\n",
    "        self.computed_IDFlistofterms = 0\n",
    "        \n",
    "    def getallfilenames(self,foldername):\n",
    "        owd = os.getcwd()\n",
    "        fld = foldername + \"/\"\n",
    "        os.chdir(fld)\n",
    "        fnamearr = []\n",
    "        for file in glob.glob(\"*.txt\"):\n",
    "            fnamearr.append(file)\n",
    "        os.chdir(owd)\n",
    "        return fnamearr        \n",
    "        \n",
    "    def getDocumentTF(self,fname):\n",
    "        file1 = open(fname)\n",
    "        readfile = file1.read()\n",
    "        wordsInFile = readfile.split()\n",
    "        counts_all = Counter(wordsInFile)\n",
    "        words, count_values = zip(*counts_all.items())\n",
    "        values_sorted, words_sorted = zip(*sorted(zip(count_values, words), key=operator.itemgetter(0), reverse=True))\n",
    "        countdf = pd.DataFrame({'count': values_sorted, 'word': words_sorted})\n",
    "        return countdf\n",
    "    \n",
    "    def computeTFmatrix(self):\n",
    "        if self.computed_tfmatrix==0:\n",
    "            owd = \"/Users/arnabborah/Documents/repositories/textclusteringDBSCAN\"\n",
    "            print(owd)\n",
    "            fld = \"/Users/arnabborah/Documents/repositories/textclusteringDBSCAN/processFiles/trainCatFiles/\"\n",
    "            os.chdir(fld)\n",
    "\n",
    "            for file in glob.glob(\"*.txt\"):\n",
    "                self.allfiles.append(\"/Users/arnabborah/Documents/repositories/textclusteringDBSCAN/processFiles/trainCatFiles/\" + file)   #this factor self.allfiles is not used, but just to record filename\n",
    "                doctf = self.getDocumentTF(file)\n",
    "                self.tfmatrixAllfiles.append(doctf)\n",
    "            os.chdir(owd)\n",
    "            self.computed_tfmatrix=1\n",
    "        else:\n",
    "            print(\"tf matrix already computed\")\n",
    "        self.nom = len(self.tfmatrixAllfiles)\n",
    "        \n",
    "    def getTermFreq(self,docId,term):\n",
    "        valx = self.tfmatrixAllfiles[docId][self.tfmatrixAllfiles[docId]['word']==term]['count'].tolist()\n",
    "        if len(valx)==0:\n",
    "            return 0\n",
    "        return valx[0]\n",
    "        \n",
    "    def computeIDFlistofterms(self):\n",
    "        totalwords = 0\n",
    "        for file in self.allfiles:\n",
    "            print(\"IDF processing...\",file)    \n",
    "            file1 = open(file)\n",
    "            readfile = file1.read()\n",
    "            wordsInFile = readfile.split()\n",
    "            lenx=len(wordsInFile)\n",
    "            totalwords+=lenx\n",
    "            self.termsforIDF.extend(set(wordsInFile))\n",
    "        print(\"Created list of terms for IDF matrix with\", len(self.termsforIDF),\" terms i.e. \",len(self.termsforIDF)/totalwords*100,\" of total words!\")\n",
    "        self.computed_IDFlistofterms = 1\n",
    "        \n",
    "    def getIdf(self,term):\n",
    "        countPresentDocs = 0\n",
    "        for i in range(self.nom):\n",
    "            tfx = self.getTermFreq(i,term)\n",
    "            if tfx>0:\n",
    "                countPresentDocs+=1\n",
    "        return countPresentDocs\n",
    "\n",
    "    def computeIDFmatrix(self):\n",
    "        if self.computed_IDFlistofterms == 0:\n",
    "            self.computeIDFlistofterms()\n",
    "        lenv = len(self.termsforIDF)\n",
    "        if self.computed_idfmatrix==0:        \n",
    "            print(\"Computing inverse document frequencies for \",lenv,\"terms!\")\n",
    "            for j in range(lenv):\n",
    "                el = self.termsforIDF[j]\n",
    "                idfx = self.getIdf(el)\n",
    "                idfy = lenv/float(1+idfx)\n",
    "                idfz = math.log(idfy,10)\n",
    "                self.idfMatrix[el] = [idfz]\n",
    "                prog=(j+1)/lenv\n",
    "                drawProgressBar(prog)\n",
    "            self.computed_idfmatrix=1\n",
    "        else:\n",
    "             print(\"IDF matrix already computed, with\", len(self.termsforIDF),\"terms!\")\n",
    "        \n",
    "    def compute_tfidf(self,fileIndex,filename):\n",
    "        tfidfFileMatrix = {}\n",
    "        file1 = open(filename)\n",
    "        readfile = file1.read()\n",
    "        wordsInFile = set(readfile.split())\n",
    "        for word in wordsInFile:\n",
    "            tf_final = self.getTermFreq(fileIndex,word)\n",
    "            idf_final = self.idfMatrix[word][0]\n",
    "            tfidf_final = tf_final * idf_final\n",
    "            tfidfFileMatrix[word] = tfidf_final\n",
    "        return tfidfFileMatrix\n",
    "        \n",
    "    def generateCategoricalCharacteristicFiles(self):\n",
    "        print(len(ft.tfidfMatrix),\"matrix files available!\")\n",
    "        for matrixID in range(len(ft.tfidfMatrix)):\n",
    "            fname_orig = allfnames[matrixID]\n",
    "            newfname = fname_orig.split()[0]+'_characteristic.csv'\n",
    "            print(\"filename:\",newfname)\n",
    "            matrixDF = pd.DataFrame.from_dict(ft.tfidfMatrix[matrixID], orient = 'index')\n",
    "            matrixDF.to_csv(newfname, header=False)\n",
    "            \n",
    "    def computeTFIDFallfiles(self,flist):\n",
    "        lenflist = len(flist)\n",
    "        if self.computed_tfmatrix == 0:\n",
    "            self.computeTFmatrix()\n",
    "        if self.computed_idfmatrix == 0:\n",
    "            self.computeIDFmatrix()        \n",
    "        for fin in range(lenflist):\n",
    "            fname = \"/Users/arnabborah/Documents/repositories/textclusteringDBSCAN/processFiles/trainCatFiles/\" + flist[fin]\n",
    "            print(\"file:\",fname)\n",
    "            tfidfMAT = self.compute_tfidf(fin,fname)\n",
    "            self.tfidfMatrix.append(tfidfMAT)\n",
    "            prog=(fin+1)/lenflist\n",
    "            drawProgressBar(prog)           \n",
    "        print(\"tfidfMatrix created for \",len(self.tfidfMatrix),\"documents!\")\n",
    "        self.generateCategoricalCharacteristicFiles()\n",
    "        \n",
    "os.chdir(\"/Users/arnabborah/Documents/repositories/textclusteringDBSCAN/scripts/\")\n",
    "\n",
    "# using both the classes declared above on a new dataset\n",
    "rp = dataProcessor(\"../datasets/DataAnalyst.csv\")\n",
    "rp.customProcessData()\n",
    "for index, row in rp.dataTopCatGrouped.iterrows():\n",
    "    ofname = '../processFiles/trainCatFiles/cat' + str(index)+ '_' + ''.join(row['Industry'].split()) + '.txt'    \n",
    "    print(\"Processing...\",ofname)\n",
    "    rp.rem_stop_punct(row['Job Description'],ofname)\n",
    "\n",
    "ft = flingCategoricalTFIDF()\n",
    "allfnames = ft.getallfilenames(\"/Users/arnabborah/Documents/repositories/textclusteringDBSCAN/processFiles/trainCatFiles\")\n",
    "ft.computeTFIDFallfiles(allfnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
