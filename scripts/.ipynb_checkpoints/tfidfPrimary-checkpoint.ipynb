{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imp import reload\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import nltk,re,pprint\n",
    "import sys,glob,os\n",
    "import operator, string, argparse, math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "swords = set(stopwords.words('english'))\n",
    "\n",
    "def getallfilenames(foldername):#returns the name of all files inside the source folder. \t\t\n",
    "    owd = os.getcwd()\n",
    "    fld = foldername + \"/\"\n",
    "    os.chdir(fld)\t\t\t\t\t#this is the name of the folder from which the file names are returned.\n",
    "    fnamearr = []\t\t\t\t\t\t#empty array, the names of files are appended to this array, and returned.\n",
    "    for file in glob.glob(\"*.txt\"):\n",
    "        fnamearr.append(file)\n",
    "    os.chdir(owd)\n",
    "    return fnamearr\n",
    "\n",
    "def drawProgressBar(percent, barLen = 50):\t\t\t#just a progress bar so that you dont lose patience\n",
    "    sys.stdout.write(\"\\r\")\n",
    "    progress = \"\"\n",
    "    for i in range(barLen):\n",
    "        if i<int(barLen * percent):\n",
    "            progress += \"=\"\n",
    "        else:\n",
    "            progress += \" \"\n",
    "    sys.stdout.write(\"[ %s ] %.2f%%\" % (progress, percent * 100))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "def rem_stop_punct(originalText):\n",
    "    splittedText = originalText.split()\n",
    "    lenl = len(splittedText)\n",
    "    wordFiltered = []\n",
    "    tSent = []\n",
    "    for r in range(lenl):\n",
    "        wordx_1 = splittedText[r]\n",
    "        wordx_2 = \"\".join(c for c in wordx_1 if c not in ('!','.',':',',','?',';','``','&','-','\"','(',')','[',']','0','1','2','3','4','5','6','7','8','9')) \n",
    "        sWord = wordx_2.lower()\n",
    "        if sWord not in swords:\n",
    "            tSent.append(sWord)\n",
    "    return \" \".join(tSent)\n",
    "            \n",
    "#allfnames = getallfilenames(\"/Users/arnabborah/Documents/repositories/textclusteringDBSCAN/processFiles/trainCatFiles\")\n",
    "#allfnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  Job Description  Company Name\n",
      "Industry                                                       \n",
      "-1                                            353           352\n",
      "IT Services                                   325           325\n",
      "Staffing & Outsourcing                        323           323\n",
      "Health Care Services & Hospitals              151           151\n",
      "Consulting                                    111           111\n",
      "...                                           ...           ...\n",
      "Chemical Manufacturing                          1             1\n",
      "Pet & Pet Supplies Stores                       1             1\n",
      "Consumer Product Rental                         1             1\n",
      "Metals Brokers                                  1             1\n",
      "News Outlet                                     1             1\n",
      "\n",
      "[89 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "class dataProcessor:\n",
    "    def __init__(self, fname, keep_factors = ['Job Description', 'Company Name', 'Industry'], group_column = 'Industry'):\n",
    "        self.dataInitial = pd.read_csv(fname, encoding=\"latin\")\n",
    "        self.dataInitialSmall = self.dataInitial[['Job Description', 'Company Name', 'Industry']]\n",
    "        self.swords = set(stopwords.words('english'))\n",
    "        #print(len(self.swords),\"stopwords present!\")\n",
    "        self.dataInitialGrouped = self.dataInitialSmall.groupby([group_column]).count()\n",
    "        pd.set_option('display.max_rows', 50)\n",
    "        print(self.dataInitialGrouped.sort_values(by=['Job Description'], ascending=False))\n",
    "\n",
    "    # pipeline for purifying the text, write-pipeline, so just output filename can be provided\n",
    "    def rem_stop_punct(self,originalText, ofilename):\n",
    "        splittedText = originalText.split()\n",
    "        lenl = len(splittedText)\n",
    "        print(\"Length is: \",lenl, splittedText[:5])\n",
    "        ofile = open(ofilename,'a')\n",
    "        \n",
    "        for r in range(lenl):\n",
    "            linex = splittedText[r]\n",
    "            linex2 = \"\".join(c for c in linex if c not in ('!','.',':',',','?',';','``','&','-','\"','(',')','[',']','0','1','2','3','4','5','6','7','8','9'))\n",
    "            linex3 = linex2.split()\n",
    "            #prog=(r+1)/len(rawlines)\n",
    "            for s in range(len(linex3)):\n",
    "                noword = linex3[s].lower()\n",
    "                if noword not in self.swords:\n",
    "                    ofile.write(noword)\n",
    "                    ofile.write(\" \")\n",
    "                        \n",
    "os.chdir(\"/Users/arnabborah/Documents/repositories/textclusteringDBSCAN/scripts/\")\n",
    "\n",
    "# using both the classes declared above on a new dataset\n",
    "rp = dataProcessor(\"../datasets/DataAnalyst.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class flingTFIDF:\n",
    "    def __init__(self,data,cname):\n",
    "        self.idfMatrix = {}\n",
    "        self.distanceMatrix = {}\n",
    "        self.termsforIDF = []\n",
    "        self.cname = cname\n",
    "        self.data = data\n",
    "        self.lenv = len(self.data)\n",
    "        \n",
    "    def smartTokenizeColumn(self):\n",
    "        self.stopsRemoved = []\n",
    "        for index, row in self.data.iterrows():\n",
    "            prog=(index+1)/self.lenv\n",
    "            originText = row[self.cname]\n",
    "            sentx = rem_stop_punct(originText)\n",
    "            #self.stopsRemoved.append(sentx)\n",
    "            drawProgressBar(prog)\n",
    "            self.data.loc[index,'stopsRemoved'] = sentx\n",
    "        #self.data['stopsRemoved'] = self.stopsRemoved\n",
    "        self.cname = 'stopsRemoved'\n",
    "        \n",
    "    def getTF(self):\n",
    "        print(\"adding term frequency column based on\",self.cname)\n",
    "        #self.tfList = []\n",
    "        #self.tfShapes = []\n",
    "        for index, row in self.data.iterrows():\n",
    "            words_in_column = row[self.cname].split()\n",
    "            if len(words_in_column)!=0:\n",
    "                counts_all = Counter(words_in_column)\n",
    "                words, count_values = zip(*counts_all.items())\n",
    "                values_sorted, words_sorted = zip(*sorted(zip(count_values, words), key=operator.itemgetter(0), reverse=True))\n",
    "                countdf = pd.DataFrame({'word': words_sorted, 'tf': values_sorted})\n",
    "                #self.tfList.append(countdf)\n",
    "                #self.tfShapes.append(countdf.shape)\n",
    "                self.data.loc[index,'tfMatrix'] = countdf\n",
    "            else:\n",
    "                #self.tfList.append(pd.DataFrame(columns = ['word','tf']))\n",
    "                #self.tfShapes.append(\"empty\")\n",
    "                self.data.loc[index,'tfMatrix'] = pd.DataFrame(columns = ['word','tf'])\n",
    "            prog=(index+1)/self.lenv\n",
    "            drawProgressBar(prog)\n",
    "        #self.data['tfShape'] = self.tfShapes\n",
    "        #self.data['tfList'] = self.tfList\n",
    "        \n",
    "    def getTFIDF(self):\n",
    "        print(\"\\nComputing and adding TF-IDF column based on\",self.cname)\n",
    "        for index, row in self.data.iterrows():\n",
    "            tfmatrixThisrow = row['tfMatrix']\n",
    "            tempTFIDF = []\n",
    "            for indx, rwx in tfmatrixThisrow.iterrows():\n",
    "                trmx = rwx['word']\n",
    "                tfx = rwx['tf']\n",
    "                idfx = self.idfMatrix[trmx]\n",
    "                tfidfx = tfx*idfx\n",
    "                #tempTFIDF.append(tfidfx)\n",
    "                tfmatrixThisrow.loc[index,'tf-idf'] = tfidfx\n",
    "            #tfmatrixThisrow['tf-idf'] = tempTFIDF\n",
    "            #sumtfidf = tfmatrixThisrow['tf-idf'].sum() \n",
    "            prog=(index+1)/self.lenv\n",
    "            drawProgressBar(prog)\n",
    "                \n",
    "    def computeIDFlistofterms(self):\n",
    "        totalwords = 0\n",
    "        print(\"\\nComputing list of words for IDF...\\n\")\n",
    "        for index, row in self.data.iterrows():\n",
    "            words_in_column = set(row[self.cname].split())  \n",
    "            for word in words_in_column:\n",
    "                if word not in self.termsforIDF:\n",
    "                    self.termsforIDF.append(word)\n",
    "                    totalwords+=1\n",
    "        print(\"Created list of terms for IDF matrix with\", totalwords,\" terms.\")     \n",
    "        \n",
    "    def getIdf(self,term):\n",
    "        countPresentDocs = 0\n",
    "        lenidf = len(self.termsforIDF)\n",
    "        for i in range(lenidf):\n",
    "            tfx = self.getTermFreq(i,term)\n",
    "            if tfx>0:\n",
    "                countPresentDocs+=1\n",
    "            prog=(i+1)/lenidf\n",
    "            drawProgressBar(prog)\n",
    "        return countPresentDocs\n",
    "        \n",
    "    def computeIDFmatrix(self):\n",
    "        self.computeIDFlistofterms()\n",
    "        print(\"\\nComputing global IDF matrix...\\n\")\n",
    "        for term in self.termsforIDF:\n",
    "            self.idfMatrix[term]=0\n",
    "        for index, row in self.data.iterrows():\n",
    "            listofterms = list(self.data['tfMatrix'][index]['word'])\n",
    "            for term in listofterms:\n",
    "                self.idfMatrix[term]=self.idfMatrix[term]+1\n",
    "            prog=(index+1)/self.lenv\n",
    "            drawProgressBar(prog)\n",
    "        for term in self.termsforIDF:\n",
    "            idfx = self.idfMatrix[term]          \n",
    "            idfy = self.lenv/float(1+idfx)\n",
    "            idfz = math.log(idfy,10)\n",
    "            self.idfMatrix[term] = idfz\n",
    "            \n",
    "    def showData(self):\n",
    "        print(self.data['tfMatrix'])\n",
    "        \n",
    "    def createDistanceMetadata(self):\n",
    "        #sumList = []\n",
    "        for index, row in self.data.iterrows():\n",
    "            tfmatrixThisrow = row['tfMatrix']\n",
    "            sumTFIDF = tfmatrixThisrow['tf-idf'].sum()\n",
    "            #sumList.append({'sumTFIDF':sumTFIDF})\n",
    "            self.data.loc[index,'sumTFIDF'] = sumTFIDF\n",
    "              \n",
    "    def distanceBtnTwoDocs(self, docId_1, docId_2):\n",
    "        listWords_1 = set(list(self.data['tfMatrix'][docId_1]['word']))\n",
    "        listWords_2 = set(list(self.data['tfMatrix'][docId_2]['word']))\n",
    "        common = listWords_1.intersection(listWords_2)\n",
    "        diff1_2 = listWords_1.difference(listWords_2)\n",
    "        diff2_1 = listWords_2.difference(listWords_1)\n",
    "        sumwt1 = self.data['sumTFIDF'][docId_1]\n",
    "        sumwt2 = self.data['sumTFIDF'][docId_2]\n",
    "        score_common, score_doc1, score_doc2 = 0,0,0\n",
    "        for word_c in common:\n",
    "            score_1 = float(self.data['tfMatrix'][docId_1].loc[self.data['tfMatrix'][docId_1]['word'] == word_c]['tf-idf'])\n",
    "            score_2 = float(self.data['tfMatrix'][docId_2].loc[self.data['tfMatrix'][docId_2]['word'] == word_c]['tf-idf'])\n",
    "            score_common += abs(score_1/float(sumwt1) - score_2/float(sumwt2))\n",
    "        for word_d12 in diff1_2:\n",
    "            score_1 = float(self.data['tfMatrix'][docId_1].loc[self.data['tfMatrix'][docId_1]['word'] == word_d12]['tf-idf'])\n",
    "            score_doc1 += score_1/float(sumwt1)\n",
    "        for word_d21 in diff2_1:\n",
    "            score_2 = float(self.data['tfMatrix'][docId_2].loc[self.data['tfMatrix'][docId_2]['word'] == word_d21]['tf-idf'])\n",
    "            score_doc2 += score_2/float(sumwt2)\n",
    "        score_total = score_common + score_doc1 + score_doc2\n",
    "        return(score_total)\n",
    "    \n",
    "    def computeDistanceBtnAllDocs(self):\n",
    "        for j in range(100):\n",
    "            for k in range(10):\n",
    "                numx = j*10+k\n",
    "                dist = self.distanceBtnTwoDocs(j,k)\n",
    "                self.distanceMatrix[(j,k)] = dist\n",
    "                prog=(numx+1)/1000\n",
    "                drawProgressBar(prog)\n",
    "                \n",
    "        print(self.distanceMatrix[:10])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 281 49 207\n"
     ]
    }
   ],
   "source": [
    "a = set(list(ftf.data['tfMatrix'][1]['word']))\n",
    "b = set(list(ftf.data['tfMatrix'][2]['word']))\n",
    "common = a.intersection(b)\n",
    "diff = a.difference(b)\n",
    "print(len(a),len(b),len(common),len(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.554869363760395"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(ftf.data['tfMatrix'][1].loc[ftf.data['tfMatrix'][1]['word'] == 'reporting']['tf-idf'].astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.847963208154384\n",
      "1.908929885185095\n",
      "1.8723360728153131\n",
      "1.8859878543870687\n",
      "0.0\n",
      "1.7567946040259788\n",
      "1.8183410691594477\n",
      "1.8871896618138921\n",
      "1.7567946040259788\n",
      "0.0\n",
      "1.8346014378548616\n",
      "1.908480190818862\n",
      "1.8183410691594477\n",
      "1.8346014378548616\n",
      "0.0\n",
      "1.8784864100019139\n"
     ]
    }
   ],
   "source": [
    "def getDiff(data, docId_1, docId_2):\n",
    "    listWords_1 = set(list(data['tfMatrix'][docId_1]['word']))\n",
    "    listWords_2 = set(list(data['tfMatrix'][docId_2]['word']))\n",
    "    common = listWords_1.intersection(listWords_2)\n",
    "    diff1_2 = listWords_1.difference(listWords_2)\n",
    "    diff2_1 = listWords_2.difference(listWords_1)\n",
    "    sumwt1 = data['sumTFIDF'][docId_1]\n",
    "    sumwt2 = data['sumTFIDF'][docId_2]\n",
    "    score_common, score_doc1, score_doc2 = 0,0,0\n",
    "    for word_c in common:\n",
    "        score_1 = float(data['tfMatrix'][docId_1].loc[data['tfMatrix'][docId_1]['word'] == word_c]['tf-idf'])\n",
    "        score_2 = float(data['tfMatrix'][docId_2].loc[data['tfMatrix'][docId_2]['word'] == word_c]['tf-idf'])\n",
    "        score_common += abs(score_1/float(sumwt1) - score_2/float(sumwt2))\n",
    "    for word_d12 in diff1_2:\n",
    "        score_1 = float(data['tfMatrix'][docId_1].loc[data['tfMatrix'][docId_1]['word'] == word_d12]['tf-idf'])\n",
    "        score_doc1 += score_1/float(sumwt1)\n",
    "    for word_d21 in diff2_1:\n",
    "        score_2 = float(data['tfMatrix'][docId_2].loc[data['tfMatrix'][docId_2]['word'] == word_d21]['tf-idf'])\n",
    "        score_doc2 += score_2/float(sumwt2)\n",
    "    score_total = score_common + score_doc1 + score_doc2\n",
    "    return(score_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primary distance module run\n",
    "ftf = flingTFIDF(rp.dataInitialSmall,'Job Description')\n",
    "ftf.smartTokenizeColumn()\n",
    "#ftf.getTF()\n",
    "#ftf.computeIDFmatrix()\n",
    "#ftf.getTFIDF()\n",
    "#ftf.createDistanceMetadata()\n",
    "#ftf.computeDistanceBtnAllDocs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     {'sumTFIDF': 811.5693284137141}\n",
      "1     {'sumTFIDF': 415.2875832148055}\n",
      "2    {'sumTFIDF': 439.81593224954963}\n",
      "3      {'sumTFIDF': 569.217930688834}\n",
      "4    {'sumTFIDF': 420.10671852390715}\n",
      "5    {'sumTFIDF': 289.92465152113436}\n",
      "6     {'sumTFIDF': 304.3672379088695}\n",
      "7    {'sumTFIDF': 190.85385731392748}\n",
      "8     {'sumTFIDF': 375.1300840846757}\n",
      "9     {'sumTFIDF': 392.1123815118852}\n",
      "Name: distanceMetadata, dtype: object\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500) \n",
    "total = ftf.data['distanceMetadata'][:10]\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftf.distanceMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
